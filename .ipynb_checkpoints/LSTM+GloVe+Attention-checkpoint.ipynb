{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'other']\n",
      "         id                                       comment_text  toxic  \\\n",
      "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
      "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
      "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
      "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
      "4  79357270  The reader here is not going by my say so for ...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  other  \n",
      "0             0        0       0       0              0      0  \n",
      "1             0        0       0       0              0      1  \n",
      "2             0        0       0       0              0      1  \n",
      "3             0        0       0       0              0      1  \n",
      "4             0        0       0       0              0      1  \n",
      "95851\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# read data\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['other'] = 1 - train[label_cols].max(axis=1)\n",
    "label_cols.append('other')\n",
    "\n",
    "print(label_cols)\n",
    "\n",
    "train['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "test['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "print(train.head())\n",
    "print(train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 200\n",
    "\n",
    "list_sentences_train = train['comment_text'].values\n",
    "list_sentences_test = test[\"comment_text\"].values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "\n",
    "y = train[label_cols].values\n",
    "\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(len(X_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 89 samples, validate on 10 samples\n",
      "Epoch 1/2\n",
      "64/89 [====================>.........] - ETA: 2s - loss: 0.6935 - acc: 0.5268 Epoch 00001: val_loss improved from inf to 0.67648, saving model to weights_base.best.hdf5\n",
      "89/89 [==============================] - 9s 102ms/step - loss: 0.6909 - acc: 0.5939 - val_loss: 0.6765 - val_acc: 0.8571\n",
      "Epoch 2/2\n",
      "64/89 [====================>.........] - ETA: 0s - loss: 0.6740 - acc: 0.8415Epoch 00002: val_loss improved from 0.67648 to 0.65417, saving model to weights_base.best.hdf5\n",
      "89/89 [==============================] - 2s 19ms/step - loss: 0.6714 - acc: 0.8587 - val_loss: 0.6542 - val_acc: 0.9286\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "LSTM_units = 50\n",
    "dense_units = 50\n",
    "dropout_rate = 0.1\n",
    "\n",
    "def get_model():\n",
    "    input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(input)\n",
    "    x = Bidirectional(LSTM(LSTM_units, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(dense_units, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(len(label_cols), activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, earlystopping]\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998\n",
      "226998\n",
      "(226998, 6)\n",
      "(226998, 6)\n",
      "(99, 7)\n",
      "(226998, 7)\n"
     ]
    }
   ],
   "source": [
    "label_cols_ini = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "sample_submission[label_cols_ini] = y_test[:, : -1]\n",
    "\n",
    "sample_submission.to_csv(\"baseline.csv\", index=False)\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
