{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "\n",
    "PATH = '../../data/'\n",
    "\n",
    "train = pd.read_csv(PATH + 'train.csv')\n",
    "test = pd.read_csv(PATH + 'test.csv')\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "APO = {\n",
    "    \"aren't\" : \"are not\",\n",
    "    \"can't\" : \"cannot\",\n",
    "    \"couldn't\" : \"could not\",\n",
    "    \"didn't\" : \"did not\",\n",
    "    \"doesn't\" : \"does not\",\n",
    "    \"don't\" : \"do not\",\n",
    "    \"hadn't\" : \"had not\",\n",
    "    \"hasn't\" : \"has not\",\n",
    "    \"haven't\" : \"have not\",\n",
    "    \"he'd\" : \"he would\",\n",
    "    \"he'll\" : \"he will\",\n",
    "    \"he's\" : \"he is\",\n",
    "    \"i'd\" : \"I would\",\n",
    "    \"i'd\" : \"I had\",\n",
    "    \"i'll\" : \"I will\",\n",
    "    \"i'm\" : \"I am\",\n",
    "    \"isn't\" : \"is not\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"i've\" : \"I have\",\n",
    "    \"let's\" : \"let us\",\n",
    "    \"mightn't\" : \"might not\",\n",
    "    \"mustn't\" : \"must not\",\n",
    "    \"shan't\" : \"shall not\",\n",
    "    \"she'd\" : \"she would\",\n",
    "    \"she'll\" : \"she will\",\n",
    "    \"she's\" : \"she is\",\n",
    "    \"shouldn't\" : \"should not\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"there's\" : \"there is\",\n",
    "    \"they'd\" : \"they would\",\n",
    "    \"they'll\" : \"they will\",\n",
    "    \"they're\" : \"they are\",\n",
    "    \"they've\" : \"they have\",\n",
    "    \"we'd\" : \"we would\",\n",
    "    \"we're\" : \"we are\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"we've\" : \"we have\",\n",
    "    \"what'll\" : \"what will\",\n",
    "    \"what're\" : \"what are\",\n",
    "    \"what's\" : \"what is\",\n",
    "    \"what've\" : \"what have\",\n",
    "    \"where's\" : \"where is\",\n",
    "    \"who'd\" : \"who would\",\n",
    "    \"who'll\" : \"who will\",\n",
    "    \"who're\" : \"who are\",\n",
    "    \"who's\" : \"who is\",\n",
    "    \"who've\" : \"who have\",\n",
    "    \"won't\" : \"will not\",\n",
    "    \"wouldn't\" : \"would not\",\n",
    "    \"you'd\" : \"you would\",\n",
    "    \"you'll\" : \"you will\",\n",
    "    \"you're\" : \"you are\",\n",
    "    \"you've\" : \"you have\",\n",
    "    \"'re\": \" are\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'll\":\" will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"tryin'\":\"trying\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inplace na\n",
      "comment text cleaned\n",
      "word count\n",
      "unique word count\n",
      "consecutive question marks\n",
      "consecutive exclamation marks\n",
      "uppercase letters\n",
      "ellipsis\n",
      "period and ellipsis\n",
      "parentheses pairs\n",
      "['id' 'comment_text' 'toxic' 'severe_toxic' 'obscene' 'threat' 'insult'\n",
      " 'identity_hate' 'comment_text_cleaned' 'word_count' 'cleaned_word_count'\n",
      " 'unique_word_count' 'cleaned_unique_word_count'\n",
      " 'consecutive_question_marks' 'cleaned_consecutive_question_marks'\n",
      " 'consecutive_exclamation_marks' 'cleaned_consecutive_exclamation_marks'\n",
      " 'uppercase_letters' 'cleaned_uppercase_letters' 'ellipsis'\n",
      " 'cleaned_ellipsis' 'period' 'cleaned_period' 'parentheses_paird'\n",
      " 'cleaned_parentheses_pair']\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "tok = TweetTokenizer()\n",
    "\n",
    "def clean(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = re.sub(r'\\n+', ' ', comment)\n",
    "    comment = re.sub('\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', '',comment) # remove leaky elements like ip,user\n",
    "    comment = re.sub('\\[\\[.*\\]', '',comment)    #removing usernames\n",
    "    comment = re.sub('[=\",]', '', comment)\n",
    "    comment = re.sub(' +', ' ', comment)\n",
    "    text = tok.tokenize(comment)\n",
    "    text = [APO[word] if word in APO else word for word in text]\n",
    "    text = tok.tokenize(' '.join(text))\n",
    "    text = [lem.lemmatize(word, 'v') for word in text]\n",
    "    text = ' '.join(text)\n",
    "    if text == '': text = 'na'\n",
    "    return text\n",
    "\n",
    "# word count\n",
    "def word_count(comment): return len(comment.split())\n",
    "# unique word count\n",
    "def unique_word_count(comment): return len(set(comment.split()))\n",
    "# find the count of consecutive question marks (i.e. ???)\n",
    "def multi_question_mark_count(comment): return len(re.findall(r'\\?{2,}', comment))\n",
    "# find the count of consecutive exclamation marks (i.e. !!!)\n",
    "def multi_exclamation_mark_count(comment): return len(re.findall(r'!{2,}', comment))\n",
    "# find the count of uppercase letters\n",
    "def uppercase_letter_count(comment): return len(re.findall(r'[A-Z]', comment))\n",
    "# count ellipsis (3 or more . (i.e. ...))\n",
    "def ellipsis_count(comment): return len(re.findall(r'\\.{3,}', comment))\n",
    "# count period and ellipsis\n",
    "def period_count(comment): return len(re.findall(r'\\.+', comment))\n",
    "# count parentheses pairs\n",
    "def parentheses_pair_count(comment): return len(re.findall(r'\\(.*\\)', comment))\n",
    "\n",
    "print('inplace na')\n",
    "train['comment_text'].fillna('na', inplace=True)\n",
    "test['comment_text'].fillna('na', inplace=True)\n",
    "\n",
    "print('comment text cleaned')\n",
    "train['comment_text_cleaned'] = train['comment_text'].apply(clean)\n",
    "test['comment_text_cleaned'] = test['comment_text'].apply(clean)\n",
    "\n",
    "print('word count')\n",
    "train['word_count'] = train['comment_text'].apply(word_count)\n",
    "test['word_count'] = test['comment_text'].apply(word_count)\n",
    "train['cleaned_word_count'] = train['comment_text_cleaned'].apply(word_count)\n",
    "test['cleaned_word_count'] = test['comment_text_cleaned'].apply(word_count)\n",
    "\n",
    "print('unique word count')\n",
    "train['unique_word_count'] = train['comment_text'].apply(unique_word_count)\n",
    "test['unique_word_count'] = test['comment_text'].apply(unique_word_count)\n",
    "train['cleaned_unique_word_count'] = train['comment_text_cleaned'].apply(unique_word_count)\n",
    "test['cleaned_unique_word_count'] = test['comment_text_cleaned'].apply(unique_word_count)\n",
    "\n",
    "print('consecutive question marks')\n",
    "train['consecutive_question_marks'] = train['comment_text'].apply(multi_question_mark_count)\n",
    "test['consecutive_question_marks'] = test['comment_text'].apply(multi_question_mark_count)\n",
    "train['cleaned_consecutive_question_marks'] = train['comment_text_cleaned'].apply(multi_question_mark_count)\n",
    "test['cleaned_consecutive_question_marks'] = test['comment_text_cleaned'].apply(multi_question_mark_count)\n",
    "\n",
    "print('consecutive exclamation marks')\n",
    "train['consecutive_exclamation_marks'] = train['comment_text'].apply(multi_exclamation_mark_count)\n",
    "test['consecutive_exclamation_marks'] = test['comment_text'].apply(multi_exclamation_mark_count)\n",
    "train['cleaned_consecutive_exclamation_marks'] = train['comment_text_cleaned'].apply(multi_exclamation_mark_count)\n",
    "test['cleaned_consecutive_exclamation_marks'] = test['comment_text_cleaned'].apply(multi_exclamation_mark_count)\n",
    "\n",
    "print('uppercase letters')\n",
    "train['uppercase_letters'] = train['comment_text'].apply(uppercase_letter_count)\n",
    "test['uppercase_letters'] = test['comment_text'].apply(uppercase_letter_count)\n",
    "train['cleaned_uppercase_letters'] = train['comment_text_cleaned'].apply(uppercase_letter_count)\n",
    "test['cleaned_uppercase_letters'] = test['comment_text_cleaned'].apply(uppercase_letter_count)\n",
    "\n",
    "print('ellipsis')\n",
    "train['ellipsis'] = train['comment_text'].apply(ellipsis_count)\n",
    "test['ellipsis'] = test['comment_text'].apply(ellipsis_count)\n",
    "train['cleaned_ellipsis'] = train['comment_text_cleaned'].apply(ellipsis_count)\n",
    "test['cleaned_ellipsis'] = test['comment_text_cleaned'].apply(ellipsis_count)\n",
    "\n",
    "print('period and ellipsis')\n",
    "train['period'] = train['comment_text'].apply(period_count)\n",
    "test['period'] = test['comment_text'].apply(period_count)\n",
    "train['cleaned_period'] = train['comment_text_cleaned'].apply(period_count)\n",
    "test['cleaned_period'] = test['comment_text_cleaned'].apply(period_count)\n",
    "\n",
    "print('parentheses pairs')\n",
    "train['parentheses_paird'] = train['comment_text'].apply(parentheses_pair_count)\n",
    "test['parentheses_paird'] = test['comment_text'].apply(parentheses_pair_count)\n",
    "train['cleaned_parentheses_pair'] = train['comment_text_cleaned'].apply(parentheses_pair_count)\n",
    "test['cleaned_parentheses_pair'] = test['comment_text_cleaned'].apply(parentheses_pair_count)\n",
    "\n",
    "print(train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train.to_csv(PATH + 'cleaned_train.csv')\n",
    "test.to_csv(PATH + 'cleaned_test.csv')\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
