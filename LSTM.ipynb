{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "         id                                       comment_text  toxic  \\\n",
      "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
      "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
      "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
      "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
      "4  79357270  The reader here is not going by my say so for ...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "95851\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# read data\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "# train['other'] = 1 - train[label_cols].max(axis=1)\n",
    "# label_cols.append('other')\n",
    "\n",
    "print(label_cols)\n",
    "\n",
    "train['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "test['comment_text'].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "print(train.head())\n",
    "print(train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 200\n",
    "\n",
    "list_sentences_train = train['comment_text'].values\n",
    "list_sentences_test = test[\"comment_text\"].values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "\n",
    "y = train[label_cols].values\n",
    "\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(len(X_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9756Epoch 00001: val_loss improved from inf to 0.05310, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 1459s 17ms/step - loss: 0.0749 - acc: 0.9756 - val_loss: 0.0531 - val_acc: 0.9809\n",
      "Epoch 2/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9825Epoch 00002: val_loss improved from 0.05310 to 0.04949, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 1434s 17ms/step - loss: 0.0477 - acc: 0.9825 - val_loss: 0.0495 - val_acc: 0.9818\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "LSTM_units = 50\n",
    "dense_units = 50\n",
    "dropout_rate = 0.1\n",
    "\n",
    "def get_model():\n",
    "    input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(input)\n",
    "    x = Bidirectional(LSTM(LSTM_units, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(dense_units, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(len(label_cols), activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, earlystopping]\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "label_cols_ini = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "sample_submission[label_cols_ini] = y_test #[:, : -1]\n",
    "\n",
    "sample_submission.to_csv(\"baseline.csv\", index=False)\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
